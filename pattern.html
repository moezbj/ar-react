<html>

<head>
    <meta charset="UTF-8">
    <title>Handpose with tfjs</title>
    <meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1.0, user-scalable=no">

    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/hand-pose-detection"></script>
    <script src="https://webrtc.github.io/adapter/adapter-latest.js"></script>

    <style>
        * {
            margin: 0;
            padding: 0
        }

        body {
            width: 100vw;
            height: 100vh;
        }

        #canvas {
            width: 100% !important;
            height: 100% !important;
        }

        #overlayLoading {
            position: absolute;
            top: 0;
            left: 0;
            background-color: rgba(100, 91, 91, 0.3);
            z-index: 5;
            width: 100%;
            height: 100%;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
        }

        #btn {
            border-radius: 5px;
            background-color: black;
            color: white;
            border: none;
            z-index: 999;
            padding: 10px 15px 10px 15px;
            margin-top: 25px;
            cursor: pointer;
        }

        #btn:hover {
            background-color: white;
            color: black;
        }

        #text {
            color: rgba(255, 255, 255, 1);
            margin-top: 10px;
            margin-bottom: 10px;
            font-size: larger;
            font-weight: 700;
        }
    </style>

</head>

<body>
    <canvas id="canvas" width=360 height=360></canvas>
    <video id="video" width=360 height=360 autoplay style="display:none"></video>
    <div id="overlayLoading" width=420 height=420>
        <img src="./models/hand.png" width="180" height="180" />
        <p id="text">Place your hand here</p>
        <button id="btn">Start</button>
    </div>
    <script>
        let video = document.getElementById("video");
        let canvas = document.getElementById("canvas");
        let ctx = canvas.getContext("2d");
        let hands;
        let detector;

        let showKeyPoints = true;
        let useFront = true;
        let image = new Image();
        image.src = "./models/henna.png";
        image.onload = () => {
            console.log("Image hand loaded successfully");
        };
        let image2 = new Image();
        image2.src = "./models/henna-left.png";
        image2.onload = () => {
            console.log("Image hand loaded successfully");
        };
        document.getElementById("btn").addEventListener("click", onclickButton);

        function startVideo() {
            if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
                let mode = useFront ? "user" : "environment";
                navigator.mediaDevices.getUserMedia({ audio: false, video: { width: 360, height: 360, facingMode: mode } }).then(function (stream) {
                    video.addEventListener("loadedmetadata", loadModel);
                    video.srcObject = stream;
                    video.play();

                    drawCameraIntoCanvas();
                    feedback("onVideoReady");
                });
            } else {
                feedback("onError", "Failed to capture video");
            }
        }


        async function loadModel() {
            const model = handPoseDetection.SupportedModels.MediaPipeHands;
            const detectorConfig = {
                runtime: 'mediapipe',
                maxHands: 2,
                solutionPath: "https://cdn.jsdelivr.net/npm/@mediapipe/hands"
            };
            detector = await handPoseDetection.createDetector(model, detectorConfig);

            feedback("onModelReady");

        }

        async function estimateHands() {
            const estimationConfig = { flipHorizontal: useFront };
            const rawHands = await detector.estimateHands(video, estimationConfig);

            hands = getKeypoints(rawHands);
            feedback("onHands", hands);

            if (showKeyPoints) {
                drawKeypoints();
            }

            window.requestAnimationFrame(estimateHands);
        }
        function drawHandOverlay() {
            ctx.clearRect(0, 0, ctx.width, ctx.height);
            if (hands.length > 0) {
                let hand = hands[0];
                console.log('hand', hand)
                let palmStartCenter = hand.keypoints[9];
                if (palmStartCenter.length) {
                    let imgWidth = 200;
                    let imgHeight = 200;
                    if (hand.handedness === "Left") {
                        ctx.drawImage(image, palmStartCenter[0] / 1000, palmStartCenter[1] / 1000, imgWidth, imgHeight);
                    }
                    else {
                        ctx.drawImage(image2, palmStartCenter[0] / 1000, palmStartCenter[1] / 1000, imgWidth, imgHeight);
                    }
                }
            }
        }
        function drawCameraIntoCanvas() {
            if (useFront) {
                ctx.clearRect(0, 0, 360, 360);
                ctx.save();
                ctx.scale(-1, 1);
                ctx.translate(-360, 0);
                ctx.drawImage(video, 0, 0, 360, 360);
                ctx.restore();
            } else {
                ctx.drawImage(video, 0, 0, 360, 360);
            }
            window.requestAnimationFrame(drawCameraIntoCanvas);
        }

        function drawKeypoints() {

            for (let i = 0; i < hands.length; i += 1) {
                for (let j = 0; j < hands[i].keypoints.length; j += 1) {
                    let keypoint = hands[i].keypoints[j];

                    if (j === 8 || j === 12 || j === 16) {
                        ctx.fillStyle = "#000";
                        ctx.beginPath();
                        ctx.arc(keypoint[0], keypoint[1], 14, 0, 2 * Math.PI);
                        ctx.fill();
                    }
                    if (j === 9) {
                        drawHandOverlay()
                    }


                }
            }
        }

        function getKeypoints(hands) {
            let keypoints = [];
            for (let i = 0; i < hands.length; i += 1) {

                let hand = {};
                let points = [];
                for (let j = 0; j < hands[i].keypoints.length; j += 1) {
                    let keypoint = hands[i].keypoints[j];
                    points.push([keypoint.x, keypoint.y]);
                }
                hand.handedness = hands[i].handedness;
                hand.keypoints = points;
                keypoints.push(hand);
            }
            return keypoints;
        }

        function feedback(eventName, message) {
            let log = {};
            log.event = eventName;
            if (message) { log.data = message; }

            if (window.AppInventor) {
                window.AppInventor.setWebViewString(JSON.stringify(log));
            } else {
                console.log(JSON.stringify(log));
            }
        }

        function onclickButton(e) {
            e.preventDefault();  // Prevent any default behavior
            e.stopPropagation();  // Prevent any default behavior
            console.log("Button clicked");
            loading = false;
            overlayLoading.style.display = "none";  // Hide the overlayCanvasHand
            estimateHands();

        }
        startVideo();



    </script>
</body>

</html>